{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final CSE 6242 Project : Weather Prediction\n",
    "\n",
    "Traditional weather prediction has been performed using physics-based models based on dynamical systems. In recent years, the abundance of atmospheric data and computing power gives way to machine learning methods. In this paper, we suggest an ensemble learning approach that combines the predictions of multiple independently trained machine learning models, specifically an Artificial Neural Network and a Deep Residual Convolutional Neural Network to predict temperature and precipitation over a month. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Humanity has long tried to predict the weather, as the weather defines the environment around us. The influences and effects brought by weather prediction are not only in choosing clothes or determining to take an umbrella, but also in the various and diverse aspects of development. Peasantry needs weather prediction to deal with the farming arrangement. Environmentalists apply weather prediction to foresee climate changes for global warming control. Meanwhile, weather changing is a crucial consideration for the physical and chemical materials used in infrastructure. Knowing the weather affects everything from trade to agriculture to all of us who check the weather apps on our phones, predicting the weather has an immense impact. \n",
    "\n",
    "Performing weather forecasting is a unique problem, as data is abundant, but there is a lot of error in standard models, as weather is inherently chaotic and unpredictable. Additionally, as there is a large amount of data, good forecasting requires vast computational power. As a result, our model- building approach will follow more of a proof of concept of some key innovations on standard methods rather than trying to compete with the state-of-the-art. We will also create a platform for visualizing our predictions in an easily interpretable way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Statement\n",
    "\n",
    "Given a historical dataset of various weather features, we predict temperature and precipitation in the United States over a one month time-frame and display our predictions using a useful visualization. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methodology\n",
    "\n",
    "We propose a novel ensemble learning approach to medium-term (up to 30 days) weather forecasting, using a variety of innovations in our technical approach. \n",
    "\n",
    "- Data Augmentation with Rotations and Averaging.\n",
    "- Decreasing spatial resolution for more accurate but generalized predictions longer than 10 days. \n",
    "- Ensemble Approach with an Artificial Neural Network and a Deep Residual Convolutional Neural Network.\n",
    "- Intuitive visualization of results and evaluations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble Learning with ANN and ResNet19\n",
    "\n",
    "We are trying to predict two main aspects of the weather, temperature and precipitation. Most state-of-the-art models forecast for the upcoming 3-5 days, and we seek to expand this range to up to 30 days. In order to do this, we propose a new approach: sacrificing spatial resolution to extend the accuracy of predictions into the future. That is, rather than maintaining the spatial resolution of our dataset, we average our data over larger location-windows. As a result, our predictions range from being primarily meteorological for shorter time-frames, to being climatological for longer forecasts. \n",
    "\n",
    "Another aspect of classical weather forecasting that has not yet been implemented in the current state of the art deep learning forecasting models is using ensemble methods. Ensemble weather forecasting continues to be the standard approach to Monte Carlo Analysis of dynamical weather systems in physics-based models. This leads to another proposed innovation: use of ensemble stacking with lead forecasting time as a parameter. Generally, this entails training multiple versions of the models, each with different spatial resolutions, as well as models with different architectures, and then performing stacking generalization using a combiner algorithm. \n",
    "\n",
    "This approach provides many advantages over standard approaches. Firstly, each of the underlying models can be made simpler, as they are part of an ensemble, and can be trained independently of each other. Then, these independently trained models can be combined for different kinds of predictions, for instance a combiner algorithm that weights lower spatial resolution would be better for longer term climatology forecasts, while one for shorter meteorological forecasts could prefer standard approaches. \n",
    "\n",
    "We will be making two models and performing a stacking ensemble over the results of these two models. One of our models will be a fully convolutional ResNet-19 model, with some modifications as we are dealing with weather data. The other model is an Artificial Neural Network with five hidden layers consisting of eight neurons each, with a tansigmoid transfer function. We will still have to do some experiments to further refine the parameters of our model. For all models, we will do direct rather than iterative forecasting, as we have many more input features than those for which we predict. \n",
    "\n",
    "The model will be trained using a rolling cross-validation method. First, we will split the data into discrete time chunks. In the first iteration, we will use between a quarter and a third of the data as training data- specifically the data chunks that are earliest in the time series- and we will use the time chunk of data immediately after the training data as our test data. In the following iteration, the test data will become part of the training data and the next time-chunk in the series will become the test data, and so on and so forth until all but one of the time-chunks are training data and the most recent time chunk is the test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
