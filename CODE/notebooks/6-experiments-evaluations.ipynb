{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiments and results:\n",
    "How did you evaluate your approaches?\n",
    "What are the results?\n",
    "How do your methods compare to other methods?\n",
    "Description of your testbed; list of questions your experiments are designed to answer\n",
    "Detailed description of the experiments; observations (as many as you can!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments and Evaluation\n",
    "\n",
    "We can compare to baseline weather prediction models. A persistence baseline is one where tomorrow's prediction is today's weather. A climatology forecast baseline averages historical data for a prediction that matches the climate of the region, but pays no attention to the dynamism of day to day weather. Finally, we can perform more standard metrics-based evaluation as is typical of machine learning models. \n",
    "\n",
    "For the prediction performance, root mean squared error and accuracy are used to evaluate between prediction values for temperature and precipitation and the weather records. Further, tracking signal is used for detecting whether bias changes in our model.\n",
    "$$\\text{RMSE} =  \\sqrt{\\frac{1}{T} \\sum_{t = 1}^{T} \\left(\\hat{y}_t - y_t \\right)^2}$$\n",
    "\n",
    "A common method to evaluate weather prediction models is root mean squared error (RMSE). A smaller RMSE is optimal because it signifies a smaller magnitude of error. As more predictions are generated for future points in time, we will be able to graph how RMSE changes. It is generally difficult to predict weather after 10 days, so we expect the RMSE to increase as more predictions are made, and we may anticipate a spike in RMSE after 10 days. Our team plans to evaluate the validity of this statement based on newer models.\n",
    "\n",
    "Accuracy could be another method to evaluate time series prediction. Besides judging from the magnitude of accuracy, the tendency of accuracy change in the long forecasting could reflect the stability of our model. Small slope of accuracy curve indicates smoothly decreasing of accuracy in terms of prediction period.   \n",
    "While RMSE captures the magnitude of error, it does not evaluate bias. In order to determine whether our model is consistently making the same type of error (e.g. the model constantly overpredicts temperature), we will also keep track of the tracking signal (TS), which is calculated by the following equation.\n",
    "\n",
    "$$\\text{Tracking Signal} =  \\frac{1}{\\text{MAD}} \\sum_{t = 1}^{T} \\left(a_t - f_t \\right)^2 $$\n",
    "\n",
    "$$\\text{MAD} =  \\frac{1}{n} \\sum_{t = 1}^{T} | \\hat{y}_t - y_t |$$\n",
    "\n",
    "A TS of greater than 4 or less than -- 4 indicates the presence of consistent bias in our predictions, so such TS's necessitate the revision of our models to be more complex. We do not foresee this being a problem, since the models we will be using are highly parameterized, but weather is a complicated phenomenon with numerous interactions, so our models may not be able to capture all of the relations and patterns between climate factors, especially as we predict weather farther from the present."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
